{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnJOKyZbeRab"
   },
   "source": [
    "# **Plant Seedlings Image Classification CNN - Computer Vision Project _Prabhu_12Feb2021**\n",
    "\n",
    "#####################################################\n",
    "\n",
    "### **Context:**\n",
    "- Can you differentiate a weed from a crop seedling?\n",
    "- The ability to do so effectively can mean better crop yields and better stewardship of the environment.\n",
    "- The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of unique plants belonging to 12 species at several growth stages\n",
    "\n",
    "### **Objective:**\n",
    "To implement the techniques learnt as a part of the course.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk4Q_z7C1l_u"
   },
   "source": [
    "# **Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1zUbfN5Y6O7"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import cv2\n",
    "import numpy as np                               # Import numpy\n",
    "import pandas as pd                               # Import numpy\n",
    "import seaborn as sns                             # Import Seaborn\n",
    "from skimage import data, io                     # Import skimage library (data - Test images and example data, io - Reading, saving, and displaying images) \n",
    "import matplotlib.pyplot as plt                  # Import matplotlib.pyplot (Plotting framework in Python.)\n",
    "%matplotlib inline\n",
    "import os                                        # This module provides a portable way of using operating system dependent functionality.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import math\n",
    "from glob import glob\n",
    "import tensorflow as tf                           # Import tensorflow\n",
    "from tensorflow.keras.models import Sequential     \n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Dropout, \n",
    "    Flatten, \n",
    "    Conv2D, \n",
    "    MaxPooling2D, \n",
    "    MaxPool2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import datasets, models, layers, optimizers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam       \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical            # convert to one-hot-encoding\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "from sklearn.model_selection import train_test_split       # Import test_train_split from sklearn    \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')        # Suppress warnings               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stE3eSRn6QVO"
   },
   "outputs": [],
   "source": [
    "# Mount Google drive so dataset can be accessed \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-98pZU0WUyyG"
   },
   "outputs": [],
   "source": [
    "#from zipfile import ZipFile\n",
    "#with ZipFile(train_path, 'r') as zip:\n",
    "#  zip.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1KZW5ZvdOlg"
   },
   "source": [
    "# **Load dataset, print shape of data, visualize the images in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq60GbXjYTEO"
   },
   "outputs": [],
   "source": [
    "trainLabel = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Labels.csv')\n",
    "print(trainLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUm3GAvvYXLV"
   },
   "outputs": [],
   "source": [
    "trainImg = np.load('/content/drive/My Drive/Colab Notebooks/images.npy')\n",
    "\n",
    "print(trainImg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwtqqtsnUzhB"
   },
   "outputs": [],
   "source": [
    "print(f\"Training image array shape:{trainImg.shape}\")\n",
    "print(f\"Training target labels:{trainLabel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHvSZ6iTUzpD"
   },
   "outputs": [],
   "source": [
    "# Check Images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(trainImg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXTZ3hb3bYN1"
   },
   "outputs": [],
   "source": [
    "#sobel = cv2.Sobel(img, cv2.CV_64F, 1, 1, ksize=5)\n",
    "#plt.imshow(sobel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp03874iUzsQ"
   },
   "outputs": [],
   "source": [
    "# Check Images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(trainImg[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roLbUuKJUzu2"
   },
   "outputs": [],
   "source": [
    "# Check Images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(trainImg[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sh4qtGXbo_h"
   },
   "outputs": [],
   "source": [
    "# Check Images trainImg [0] -- cv2.Sobel\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(trainImg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA9rxPtyUzxl"
   },
   "outputs": [],
   "source": [
    "#sobel = cv2.Sobel(img, cv2.CV_64F, 1, 1, ksize=5)\n",
    "#plt.imshow(sobel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YwnkbC2Vx1e"
   },
   "source": [
    "#**Pre-processing & Normalizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZgVC_gRUz0O"
   },
   "outputs": [],
   "source": [
    "trainImg = trainImg.astype('float32')\n",
    "trainImg /= 255\n",
    "# Check the nomalized data\n",
    "print(f'Shape of the Train array:{trainImg.shape}')\n",
    "print(f'Minimum value in the Train Array:{trainImg.min()}')\n",
    "print(f'Maximum value in the Train Array:{trainImg.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbZPMQ4QUz3L"
   },
   "outputs": [],
   "source": [
    "# Step#1: Split train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainImg, trainLabel, test_size=0.3, random_state=42)  \n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PsmWTytUz6m"
   },
   "outputs": [],
   "source": [
    "# Step#2: Split validation from test set \n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "X_test.shape, X_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MypDtu_5WK0G"
   },
   "source": [
    "#**One Hot Encoding to Target Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIkCMOfGUz9C"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)\n",
    "y_validation = encoder.fit_transform(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hxNJnPTU0AI"
   },
   "outputs": [],
   "source": [
    "# Display target variable\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E-aHICQWehL"
   },
   "source": [
    "#**Gaussian Blurring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1GSfVjxWbEP"
   },
   "outputs": [],
   "source": [
    "# Preview the image before Gaussian Blur\n",
    "plt.imshow(X_train[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6v_yJ4UWbHl"
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.GaussianBlur(X_train[1], (15,15), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWT5DOw9WbKX"
   },
   "outputs": [],
   "source": [
    "# Now we apply the gaussian blur to each 128x128 pixels array (image) to reduce the noise in the image\n",
    "for idx, img in enumerate(X_train):\n",
    "  X_train[idx] = cv2.GaussianBlur(img, (5, 5), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enz8c_8VWbNT"
   },
   "outputs": [],
   "source": [
    "# Preview the image after Gaussian Blur\n",
    "plt.imshow(X_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NXSGkg6WbQQ"
   },
   "outputs": [],
   "source": [
    "# Gaussian Blur to Test and Validation sets\n",
    "for idx, img in enumerate(X_test):\n",
    "  X_test[idx] = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "for idx, img in enumerate(X_validation):\n",
    "  X_validation[idx] = cv2.GaussianBlur(img, (5, 5), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugUVVVZ4XAWY"
   },
   "source": [
    "# **Creating  a CNN Model**\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Initialize CNN Classifier\n",
    "- Add Convolution layer with 32 kernels of 3x3 shape\n",
    "- Add Maxpooling layer of size 2x2\n",
    "- Flatten the input array\n",
    "- Add dense layer with relu activation function\n",
    "- Dropout the probability\n",
    "- Add softmax Dense layer as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLGrmyh_WbTM"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "  # Initialize CNN Classified\n",
    "  model = Sequential()\n",
    "\n",
    "  # Add convolution layer with 32 filters and 3 kernels\n",
    "  model.add(Conv2D(32, (3,3), input_shape=input_shape, padding='same', activation=tf.nn.relu))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(Dropout(rate=0.25))\n",
    "\n",
    "  # Add convolution layer with 32 filters and 3 kernels\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu))\n",
    "  model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation=tf.nn.relu))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(Dropout(rate=0.25))\n",
    "\n",
    "  # Add convolution layer with 32 filters and 3 kernels\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(Dropout(rate=0.25))\n",
    "\n",
    "  # Flatten the 2D array to 1D array\n",
    "  model.add(Flatten())\n",
    "\n",
    "  # Create fully connected layers with 512 units\n",
    "  model.add(Dense(512, activation=tf.nn.relu))\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "  # Adding a fully connected layer with 128 neurons\n",
    "  model.add(Dense(units = 128, activation = tf.nn.relu))\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  # The final output layer with 12 neurons to predict the categorical classifcation\n",
    "  model.add(Dense(units = num_classes, activation = tf.nn.softmax))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pEZLz4jWbXb"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.95):\n",
    "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9MpX4GuWbab"
   },
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:] # Input shape of X_train\n",
    "num_classes = y_train.shape[1] # Target column size\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Optimizer\n",
    " #optimizer = tf.keras.optimizers.SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvmzMXWWWbdC"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=10, batch_size=500, callbacks=[callbacks]) # Initial trial with epochs=10 and batch size=500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgjKEpnnWbgN"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel(r'Loss', fontsize=18)\n",
    "plt.legend(('loss train','loss validation'), loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bT3ipHkWbip"
   },
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel(r'Accuracy', fontsize=18)\n",
    "plt.legend(('accuracy train','accuracy validation'), loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm7mshn9XnfM"
   },
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5qpYZJ9Wblp"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss: {:.2f} \\n Test accuracy: {:.2f}'.format(loss, accuracy))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Train loss: {:.2f} \\n Train accuracy: {:.2f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aG42KB5pa6b"
   },
   "source": [
    "# **Retraining the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxdYoUoePCqv"
   },
   "source": [
    "- Above try with epochs=10 and batch size=500 resulted in test accuracy = 0.54, validation accuracy of 0.57 which is low and loss is high  -- -shall retry and train model  \n",
    "- Try and retrain \"model1\" with different epochs= 30 and batch size = 100 to see if test and validation accurracy increases and loss decreases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcCUZf4lXs87"
   },
   "outputs": [],
   "source": [
    "# Retrain as model1\n",
    "model1 = create_model(input_shape, num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Optimizer\n",
    "\n",
    "model1.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0hRmohGXs_s"
   },
   "outputs": [],
   "source": [
    "# Above try with epochs=10 and batch size=500  resulted in val accuracy of 0.45 which is not good - shall retry with \n",
    "#try with different epochs= 30 and batch size = 100 val accurracy increases significantly to 0.78    or 78% which is good\n",
    "history = model1.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=30, batch_size=100, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raEHN3a4MRQa"
   },
   "outputs": [],
   "source": [
    "# Print Loss ( Model1) -- retrained model\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel(r'Loss', fontsize=18)\n",
    "plt.legend(('loss train','loss validation'), loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE4_pSoVMXyo"
   },
   "outputs": [],
   "source": [
    "# Print Accuracy ( Model1) -- retrained model\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel(r'Accuracy', fontsize=18)\n",
    "plt.legend(('accuracy train','accuracy validation'), loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIzJfc9ycgb6"
   },
   "source": [
    "**Model Evaluation after re-training (model1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adoj39ieXtCY"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model1.evaluate(X_test, y_test)\n",
    "print('Test loss: {:.2f} \\n Test accuracy: {:.2f}'.format(loss, accuracy))\n",
    "\n",
    "loss, accuracy = model1.evaluate(X_train, y_train)\n",
    "print('Train loss: {:.2f} \\n Train accuracy: {:.2f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRGRhbB4Qe5G"
   },
   "source": [
    "- From above retained \"model1\" test accuracy = 0.82 validation accuracy = 0.96 increased & loss decreased --- which is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZXW2Zovqka_"
   },
   "source": [
    "# **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_D0TBo8qhYp"
   },
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiqXGuN4qhg5"
   },
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEje1o-lqhmo"
   },
   "outputs": [],
   "source": [
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQGbkkHjrCXk"
   },
   "source": [
    "- Precision: Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
    "- Recall: Out of all the positive classes, how much we predicted correctly. It should be high as possible.\n",
    "- F1-Score: F1 Score is the weighted average of Precision and Recall. \n",
    "\n",
    "Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W53uBHIrOl-"
   },
   "source": [
    "# **Visualize predictions for x_test[2], x_test[3], x_test[33], x_test[36], x_test[59]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJ4GRWzVq5iP"
   },
   "outputs": [],
   "source": [
    "y_pred = encoder.inverse_transform(y_pred)\n",
    "\n",
    "index = 2\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "print(\"Predicted label:\", y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TslS-4BMqhpm"
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "print(\"Predicted label:\", y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z_WIQbPqhst"
   },
   "outputs": [],
   "source": [
    "index = 33\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "print(\"Predicted label:\", y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-l06ntYqhvK"
   },
   "outputs": [],
   "source": [
    "index = 36\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "print(\"Predicted label:\", y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRQzSwJNqhyU"
   },
   "outputs": [],
   "source": [
    "index = 59\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "print(\"Predicted label:\", y_pred[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V2D211lsUe8"
   },
   "source": [
    "**Model Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8V9eh1dvsP77"
   },
   "outputs": [],
   "source": [
    "y_pred    # Model prediction below array shows all predicted species ( seedlings and weeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qaae2OvNXSOp"
   },
   "source": [
    "#  Above \"y_pred\" array shows all predicted species (seedlings and weeds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQDgDXLueYhk"
   },
   "source": [
    "********************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2bo9pNubX-0"
   },
   "source": [
    "# **All below project steps and tasks were achieved sucessfully**\n",
    "1. Import the libraries, load dataset, print shape of data, visualize the images in dataset. (5 Marks)\n",
    "2. Data Pre-processing: (15 Marks)\n",
    "a. Normalization.\n",
    "b. Gaussian Blurring.\n",
    "c. Visualize data after pre-processing.\n",
    "3. Make data compatible: (10 Marks)\n",
    "a. Convert labels to one-hot-vectors.\n",
    "b. Print the label for y_train[0].\n",
    "c. Split the dataset into training, testing, and validation set.\n",
    "(Hint: First split images and labels into training and testing set with test_size = 0.3. Then further split test data\n",
    "into test and validation set with test_size = 0.5)\n",
    "d. Check the shape of data, Reshape data into shapes compatible with Keras models if it’s not already. If it’s\n",
    "already in the compatible shape, then comment in the notebook that it’s already in compatible shape.\n",
    "4. Building CNN: (15 Marks)\n",
    "a. Define layers.\n",
    "b. Set optimizer and loss function. (Use Adam optimizer and categorical crossentropy.)\n",
    "5. Fit and evaluate model and print confusion matrix. (10 Marks)\n",
    "6. Visualize predictions for x_test[2], x_test[3], x_test[33], x_test[36], x_test[59]. (5 Marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx1WYTPRedX1"
   },
   "source": [
    "********************************************************************************"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Rev5 _Computer Vision  Project _Prabhu_12Feb2021",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
